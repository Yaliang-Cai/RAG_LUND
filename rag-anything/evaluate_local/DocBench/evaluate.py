#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
DocBench Evaluation Script for RAG-Anything Local (Manual Server Mode)
======================================================================

ä½¿ç”¨æµç¨‹ï¼š
---------
1. æ‰‹åŠ¨å¯åŠ¨ InternVL2-26B-AWQ æœåŠ¡ï¼ˆç«¯å£ 8001ï¼‰
   cd /data/y50056788/Yaliang/projects/lightrag
   bash start_server_internvl2.sh

2. ç”Ÿæˆç³»ç»Ÿç­”æ¡ˆï¼ˆå¯åå°è¿è¡Œï¼‰
   python evaluate.py --mode generate
   æˆ–ï¼šnohup python evaluate.py --mode generate > run_generate.log 2>&1 &

3. åœæ­¢ InternVL2-26B-AWQï¼Œå¯åŠ¨ Qwen2.5-32Bï¼ˆç«¯å£ 8002ï¼‰
   # åœ¨serverç»ˆç«¯æŒ‰ Ctrl+C
   bash start_server_qwen2.5_32b_awq.sh

4. è¯„ä¼°ç­”æ¡ˆ
   python evaluate.py --mode evaluate

5. æŸ¥çœ‹ç»Ÿè®¡
   python evaluate.py --mode stats
"""

import os

# ===== é™åˆ¶ MinerU å†…éƒ¨ vLLM çš„ GPU å†…å­˜å ç”¨ =====
# 0.10 = 4.74GB, 0.15 = 7.1GB, 0.20 = 9.47GB
# æ¨è 0.15ï¼šç»™ MinerU 7.1GBï¼ŒGPU 0 æ€»å ç”¨ ~12GB
os.environ['MINERU_VLLM_GPU_MEMORY_UTILIZATION'] = '0.15'
# =================================================

import json
import asyncio
import logging
import re
import sys
from pathlib import Path
from typing import Any
from openai import AsyncOpenAI

# Add parent directory to path to import local_rag
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from raganything.services.local_rag import LocalRagService, LocalRagSettings

# ==========================================
# é…ç½®åŒºï¼ˆç»å¯¹è·¯å¾„ï¼Œæ–¹ä¾¿åœ¨ä»»æ„ç›®å½•è¿è¡Œï¼‰
# ==========================================

# è„šæœ¬æ‰€åœ¨ç›®å½•
SCRIPT_DIR = Path("/data/y50056788/Yaliang/projects/rag-anything/evaluate_local/DocBench")

# DocBench æ•°æ®é›†ç›®å½•
DATA_ROOT = Path("/data/y50056788/Yaliang/datasets_for_eval/data_for_DocBench")

# è¾“å‡ºæ ¹ç›®å½•ï¼ˆæ‰€æœ‰ç»“æœä¿å­˜åœ¨è¿™é‡Œï¼‰
OUTPUT_DIR = SCRIPT_DIR / "docbench_results"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
PROMPT_DUMP_DIR = OUTPUT_DIR / "prompt_dumps"
PROMPT_DUMP_DIR.mkdir(parents=True, exist_ok=True)
FINAL_MESSAGES_DUMP_DIR = OUTPUT_DIR / "final_vlm_messages"
FINAL_MESSAGES_DUMP_DIR.mkdir(parents=True, exist_ok=True)

# RAG å·¥ä½œç›®å½•ï¼ˆæ¯ä¸ªæ–‡æ¡£ä¸€ä¸ªç‹¬ç«‹å›¾è°±ï¼‰
# è¾“å‡ºï¼šdocbench_results/rag_workspaces/docbench_0/, docbench_1/, ...
WORKING_DIR_ROOT = OUTPUT_DIR / "rag_workspaces"
WORKING_DIR_ROOT.mkdir(parents=True, exist_ok=True)

# MinerU è¾“å‡ºç›®å½•ï¼ˆæ¯ä¸ªæ–‡æ¡£ç‹¬ç«‹ï¼‰
# è¾“å‡ºï¼šdocbench_results/mineru_outputs/docbench_0/{pdf_name}/hybrid_auto/, ...
OUTPUT_MD_DIR = OUTPUT_DIR / "mineru_outputs"
OUTPUT_MD_DIR.mkdir(parents=True, exist_ok=True)

# API é…ç½®
RAG_API_BASE = "http://localhost:8001/v1"      # InternVL2-26B-AWQï¼ˆç”Ÿæˆç­”æ¡ˆï¼‰
JUDGE_API_BASE = "http://localhost:8002/v1"    # Qwen2.5-32Bï¼ˆè¯„ä¼°ï¼‰
RAG_API_KEY = "EMPTY"

RAG_MODEL_NAME = "OpenGVLab/InternVL2-26B-AWQ"
JUDGE_MODEL_NAME = "Qwen/Qwen2.5-32B-Instruct"

# Query å‚æ•°ï¼ˆDocBench ä¼˜åŒ–ï¼‰
DOCBENCH_QUERY_PARAMS = {
    "mode": "hybrid",
    "top_k": 30,
    "chunk_top_k": 50,
}

# ==========================================
# æ—¥å¿—é…ç½®
# ==========================================

logging.basicConfig(
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    level=logging.INFO,
    handlers=[
        logging.StreamHandler(sys.stdout),  # ç¡®ä¿è¾“å‡ºåˆ°ç»ˆç«¯
    ]
)
logger = logging.getLogger(__name__)

# åŒæ—¶é…ç½®å…¶ä»–æ¨¡å—çš„ loggerï¼Œç¡®ä¿èƒ½çœ‹åˆ°æ‰€æœ‰æ—¥å¿—
logging.getLogger("raganything").setLevel(logging.INFO)
logging.getLogger("raganything.processor").setLevel(logging.INFO)
logging.getLogger("raganything.parser").setLevel(logging.INFO)


def _extract_reference_lines(raw_prompt: str) -> list[str]:
    """
    Extract lines under "Reference Document List" from raw prompt.
    """
    marker = (
        "Reference Document List (Each entry starts with a [reference_id] "
        "that corresponds to entries in the Document Chunks):"
    )
    marker_pos = raw_prompt.rfind(marker)
    if marker_pos < 0:
        return []

    # Parse only the fenced block after the final Reference Document List marker.
    tail = raw_prompt[marker_pos + len(marker) :]
    block_match = re.search(r"```(?:[a-zA-Z0-9_+-]+)?\s*\n(.*?)\n```", tail, re.DOTALL)
    if not block_match:
        return []

    return [line.strip() for line in block_match.group(1).splitlines() if line.strip()]


def _dump_raw_prompt(
    doc_name: str,
    qa_idx: int,
    question: str,
    raw_prompt: str,
) -> Path:
    """
    Dump raw prompt for debugging reference construction.
    """
    doc_dump_dir = PROMPT_DUMP_DIR / f"docbench_{doc_name}"
    doc_dump_dir.mkdir(parents=True, exist_ok=True)
    out_file = doc_dump_dir / f"q{qa_idx + 1:03d}_raw_prompt.txt"

    ref_lines = _extract_reference_lines(raw_prompt)
    header_lines = [
        f"# doc_id: {doc_name}",
        f"# question_index: {qa_idx + 1}",
        f"# question: {question}",
        f"# reference_line_count: {len(ref_lines)}",
        "# reference_lines:",
    ]
    if ref_lines:
        header_lines.extend([f"#   {line}" for line in ref_lines])
    else:
        header_lines.append("#   <not found>")
    header_lines.extend(["", "===== RAW PROMPT START =====", ""])

    with open(out_file, "w", encoding="utf-8") as f:
        f.write("\n".join(header_lines))
        f.write(raw_prompt)

    return out_file


def _extract_reference_lines_from_messages(messages_payload: list[dict[str, Any]]) -> list[str]:
    """
    Extract reference lines from the final chat.completions messages payload.
    """
    user_text_parts: list[str] = []
    for msg in messages_payload:
        if not isinstance(msg, dict) or msg.get("role") != "user":
            continue
        content = msg.get("content")
        if isinstance(content, list):
            for item in content:
                if isinstance(item, dict) and item.get("type") == "text":
                    text = item.get("text")
                    if text:
                        user_text_parts.append(str(text))
        elif isinstance(content, str):
            user_text_parts.append(content)

    if not user_text_parts:
        return []

    return _extract_reference_lines("\n".join(user_text_parts))


def _dump_final_messages(
    doc_name: str,
    qa_idx: int,
    question: str,
    captured_calls: list[dict[str, Any]],
) -> Path:
    """
    Dump captured chat.completions messages for this query.
    """
    doc_dump_dir = FINAL_MESSAGES_DUMP_DIR / f"docbench_{doc_name}"
    doc_dump_dir.mkdir(parents=True, exist_ok=True)
    out_file = doc_dump_dir / f"q{qa_idx + 1:03d}_final_messages.json"

    selected_messages = (
        captured_calls[-1].get("messages", [])
        if captured_calls
        else []
    )
    ref_lines = _extract_reference_lines_from_messages(selected_messages)
    payload = {
        "doc_id": doc_name,
        "question_index": qa_idx + 1,
        "question": question,
        "capture_count": len(captured_calls),
        "selected_client": captured_calls[-1].get("client") if captured_calls else None,
        "reference_line_count": len(ref_lines),
        "reference_lines": ref_lines,
        "messages": selected_messages,
        "captured_calls": captured_calls,
    }
    with open(out_file, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)

    return out_file


def _set_doc_workspace(workspace: str) -> None:
    """
    Force per-document LightRAG namespace isolation.
    """
    os.environ["WORKSPACE"] = workspace
    try:
        from lightrag.kg.shared_storage import set_default_workspace

        set_default_workspace(workspace)
    except Exception as exc:
        logger.warning(f"Failed to set LightRAG workspace '{workspace}': {exc}")


def _bridge_lightrag_logs_to_run_file() -> None:
    """
    LightRAG uses its own logger with propagate=False and a console-only handler.
    Attach root file handlers so LightRAG warnings/info are persisted in run_*.log.
    """
    try:
        from lightrag.utils import logger as lightrag_logger
    except Exception as exc:
        logger.warning(f"Failed to import lightrag logger: {exc}")
        return

    root_logger = logging.getLogger()
    file_handlers = [
        h for h in root_logger.handlers if isinstance(h, logging.FileHandler)
    ]
    if not file_handlers:
        logger.warning("No root file handler found. LightRAG logs may be console-only.")
        return

    existing_files = {getattr(h, "baseFilename", None) for h in lightrag_logger.handlers}
    attached = 0
    for handler in file_handlers:
        file_path = getattr(handler, "baseFilename", None)
        if file_path and file_path not in existing_files:
            lightrag_logger.addHandler(handler)
            attached += 1

    lightrag_logger.setLevel(logging.INFO)
    logger.info(f"LightRAG log bridge ready (attached file handlers: {attached}).")


# ==========================================
# Step 1: Generate Answers
# ==========================================

async def generate_answers(
    start_id: int = 0,
    end_id: int = 229,
    resume: bool = True,
    dump_raw_prompt: bool = False,
    dump_final_messages: bool = False,
):
    """
    ä¸º DocBench ç”Ÿæˆç³»ç»Ÿç­”æ¡ˆ
    
    å‰æï¼šInternVL2-26B-AWQ æœåŠ¡å·²åœ¨ 8001 ç«¯å£è¿è¡Œ
    
    Args:
        start_id: èµ·å§‹æ–‡æ¡£ ID
        end_id: ç»“æŸæ–‡æ¡£ IDï¼ˆä¸åŒ…å«ï¼‰
        resume: æ˜¯å¦è·³è¿‡å·²å¤„ç†çš„æ–‡æ¡£
        dump_raw_prompt: æ˜¯å¦è½ç›˜æ¯é¢˜ raw_promptï¼ˆç”¨äºè°ƒè¯•å¼•ç”¨åˆ—è¡¨ï¼‰
        dump_final_messages: æ˜¯å¦è½ç›˜æœ€ç»ˆå‘é€ç»™ VLM çš„ messagesï¼ˆç”¨äºæ£€æŸ¥å¼•ç”¨åˆ—è¡¨ï¼‰
    """
    output_file = OUTPUT_DIR / "system_answers.jsonl"
    
    # æ˜¾å¼é…ç½®è·¯å¾„ï¼Œä¿è¯è¯„æµ‹æ—¶æ¯ä¸ªæ–‡æ¡£ç›®å½•å’Œæ—¥å¿—ç›®å½•å¯æ§
    settings = LocalRagSettings.from_env()
    settings.working_dir_root = str(WORKING_DIR_ROOT)
    settings.output_dir = str(OUTPUT_MD_DIR)
    settings.log_dir = str(SCRIPT_DIR / "logs")
    settings.vllm_api_base = settings.vision_vllm_api_base = RAG_API_BASE
    settings.vllm_api_key = settings.vision_vllm_api_key = RAG_API_KEY
    
    # ç”±äº GPU 1 ä¸å¯è®¿é—®ï¼Œæ‰€æœ‰æ¨¡å‹éƒ½åœ¨ GPU 0
    # GPU 0 åˆ†é…ï¼š
    # - å…¶ä»–è¿›ç¨‹: ~13 GiB
    # - MinerU: ~2.4 GiB (0.05 * 47.35)
    # - Embedding/Rerank: ~5 GiB
    # - æ€»è®¡: ~20 GiB (åº”è¯¥å¤Ÿç”¨ï¼Œè™½ç„¶ç´§å¼ )
    settings.device = "cuda:0"
    settings.llm_model_name = settings.vision_model_name = RAG_MODEL_NAME
    settings.temperature = 0.0
    settings.query_max_tokens = 2048
    settings.ingest_max_tokens = 8192
    settings.vlm_max_images = 10
    settings.vlm_enable_json_schema = True
    
    # åªåˆ›å»ºä¸€æ¬¡ service
    service = LocalRagService(settings)
    _bridge_lightrag_logs_to_run_file()
    logger.info(f"âœ… RAG Service initialized")
    
    # åŠ è½½å·²å¤„ç†çš„æ–‡æ¡£ï¼ˆç”¨äºæ–­ç‚¹ç»­ä¼ ï¼‰
    processed_docs = set()
    if resume and output_file.exists():
        with open(output_file, 'r', encoding='utf-8') as f:
            for line in f:
                data = json.loads(line)
                processed_docs.add(data['doc_id'])
        logger.info(f"ğŸ“ Resume: found {len(processed_docs)} processed documents")
    
    logger.info(f"\n{'='*80}")
    logger.info(f"ğŸ“ Generating answers for documents {start_id} to {end_id-1}")
    logger.info(f"ğŸ“‚ Output: {output_file}")
    logger.info(f"{'='*80}\n")
    
    # éå†æ¯ä¸ªæ–‡æ¡£
    with open(output_file, 'a', encoding='utf-8') as f_out:
        for doc_id in range(start_id, end_id):
            doc_name = str(doc_id)
            
            # è·³è¿‡å·²å¤„ç†
            if resume and doc_name in processed_docs:
                logger.info(f"â­ï¸  [{doc_id}] Already processed, skipping")
                continue
            
            # æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨
            folder_path = DATA_ROOT / doc_name
            if not folder_path.exists():
                logger.warning(f"âš ï¸  [{doc_id}] Folder not found: {folder_path}")
                continue
            
            # æŸ¥æ‰¾ PDF å’Œ QA æ–‡ä»¶
            pdf_file = None
            qa_file = None
            for file in folder_path.iterdir():
                if file.suffix == '.pdf':
                    pdf_file = file
                elif file.name.endswith('_qa.jsonl'):
                    qa_file = file
            
            if not pdf_file or not qa_file:
                logger.warning(f"âš ï¸  [{doc_id}] Missing PDF or QA file")
                continue
            
            logger.info(f"\n{'='*80}")
            logger.info(f"ğŸ“„ [{doc_id}/{end_id-1}] Processing: {pdf_file.name}")
            logger.info(f"{'='*80}")
            
            try:
                # æ¯ä¸ªæ–‡æ¡£ç‹¬ç«‹çš„ ID
                rag_doc_id = f"docbench_{doc_name}"
                _set_doc_workspace(rag_doc_id)
                logger.info(f"ğŸ”§ Processing: {pdf_file.name} â†’ doc_id: {rag_doc_id}")
                
                # MinerU ç‹¬ç«‹è¾“å‡ºç›®å½•ï¼ˆé¿å… PDF é‡åå†²çªï¼‰
                doc_output_dir = str(OUTPUT_MD_DIR / f"docbench_{doc_name}")
                
                # Ingest æ–‡æ¡£
                logger.info(f"ğŸ“– Ingesting document...")
                returned_doc_id = await service.ingest(
                    file_path=str(pdf_file),
                    output_dir=doc_output_dir,  # æ¯ä¸ªæ–‡æ¡£ç‹¬ç«‹ MinerU è¾“å‡º
                    doc_id=rag_doc_id
                )
                logger.info(f"âœ… Ingestion complete, doc_id: {returned_doc_id}")
                
                # åŠ è½½é—®é¢˜
                with open(qa_file, 'r', encoding='utf-8') as f_qa:
                    qa_list = [json.loads(line) for line in f_qa]
                
                logger.info(f"\nâ“ Answering {len(qa_list)} questions...")
                
                # å›ç­”æ¯ä¸ªé—®é¢˜
                for qa_idx, qa_item in enumerate(qa_list):
                    question = qa_item['question']
                    logger.info(f"  [{qa_idx+1}/{len(qa_list)}] {question[:60]}...")
                    
                    try:
                        if dump_raw_prompt:
                            try:
                                from lightrag.base import QueryParam

                                rag = await service.get_rag(rag_doc_id)
                                raw_prompt_param = QueryParam(
                                    mode=DOCBENCH_QUERY_PARAMS["mode"],
                                    top_k=DOCBENCH_QUERY_PARAMS["top_k"],
                                    chunk_top_k=DOCBENCH_QUERY_PARAMS["chunk_top_k"],
                                    only_need_prompt=True,
                                )
                                raw_prompt_result = await rag.lightrag.aquery(
                                    question, param=raw_prompt_param
                                )
                                raw_prompt_text = (
                                    raw_prompt_result.content
                                    if hasattr(raw_prompt_result, "content")
                                    else str(raw_prompt_result)
                                )
                                dump_path = _dump_raw_prompt(
                                    doc_name, qa_idx, question, raw_prompt_text
                                )
                                ref_count = len(_extract_reference_lines(raw_prompt_text))
                                logger.info(
                                    f"      ğŸ” Prompt dumped: {dump_path} (reference lines: {ref_count})"
                                )
                            except Exception as dump_exc:
                                logger.warning(f"      âš ï¸ Raw prompt dump failed: {dump_exc}")

                        captured_calls: list[dict[str, Any]] = []
                        patched_completions: list[tuple[Any, Any]] = []
                        if dump_final_messages:
                            def _patch_client(client_name: str, client_obj: Any) -> None:
                                if client_obj is None:
                                    return
                                chat_obj = getattr(client_obj, "chat", None)
                                completions_api = getattr(chat_obj, "completions", None)
                                if completions_api is None:
                                    return
                                for api_obj, _ in patched_completions:
                                    if api_obj is completions_api:
                                        return

                                original_create = completions_api.create

                                async def _capture_create(
                                    *args,
                                    _orig=original_create,
                                    _client=client_name,
                                    **kwargs,
                                ):
                                    messages_payload = kwargs.get("messages")
                                    if isinstance(messages_payload, list):
                                        captured_calls.append(
                                            {
                                                "client": _client,
                                                "messages": messages_payload,
                                            }
                                        )
                                    return await _orig(*args, **kwargs)

                                completions_api.create = _capture_create
                                patched_completions.append((completions_api, original_create))

                            _patch_client("vision", getattr(service, "vision_client", None))
                            _patch_client("text", getattr(service, "text_client", None))

                        # ä½¿ç”¨ RAG æŸ¥è¯¢
                        try:
                            answer = await service.query(
                                doc_id=rag_doc_id,
                                query=question,
                                **DOCBENCH_QUERY_PARAMS
                            )
                        finally:
                            if dump_final_messages:
                                for completions_api, original_create in patched_completions:
                                    completions_api.create = original_create

                        if dump_final_messages:
                            if captured_calls:
                                dump_path = _dump_final_messages(
                                    doc_name, qa_idx, question, captured_calls
                                )
                                final_messages = captured_calls[-1].get("messages", [])
                                ref_count = len(
                                    _extract_reference_lines_from_messages(final_messages)
                                )
                                logger.info(
                                    f"      ğŸ§¾ Final messages dumped: {dump_path} (calls: {len(captured_calls)}, reference lines: {ref_count})"
                                )
                            else:
                                logger.warning(
                                    "      âš ï¸ Final messages not captured for this query."
                                )
                        
                        logger.info(f"      âœ“ Answer: {answer[:80]}...")
                        
                        # ä¿å­˜ç»“æœï¼ˆç¬¦åˆ DocBench æ ¼å¼ï¼‰
                        result = {
                            'doc_id': doc_name,                    # æ–‡æ¡£IDï¼ˆç”¨äºåŒ¹é…ï¼‰
                            'question': question,                   # é—®é¢˜
                            'sys_ans': answer,                      # ç³»ç»Ÿç­”æ¡ˆ
                            'ref_ans': qa_item['answer'],          # å‚è€ƒç­”æ¡ˆ
                            'type': qa_item['type'],                # é—®é¢˜ç±»å‹
                            'evidence': qa_item['evidence'],        # è¯æ®æ–‡æœ¬
                        }
                        f_out.write(json.dumps(result, ensure_ascii=False) + '\n')
                        f_out.flush()
                        
                    except Exception as e:
                        logger.error(f"      âœ— Error: {e}")
                        # ä¿å­˜ç©ºç­”æ¡ˆ
                        result = {
                            'doc_id': doc_name,
                            'question': question,
                            'sys_ans': "",
                            'ref_ans': qa_item['answer'],
                            'type': qa_item['type'],
                            'evidence': qa_item['evidence'],
                        }
                        f_out.write(json.dumps(result, ensure_ascii=False) + '\n')
                        f_out.flush()
                
                logger.info(f"âœ… [{doc_id}] Completed: {len(qa_list)} questions answered\n")
                
                # æ¸…ç†å½“å‰æ–‡æ¡£çš„ RAG å®ä¾‹ï¼Œé‡Šæ”¾å†…å­˜
                # æ³¨æ„ï¼šå¿…é¡» await finalize_storages() è€Œä¸æ˜¯è°ƒç”¨ close()
                # å› ä¸º close() åœ¨ async ä¸Šä¸‹æ–‡ä¸­ä¼šå¼‚æ­¥è°ƒåº¦æ¸…ç†ï¼Œä¸ä¼šç­‰å¾…å®Œæˆ
                if rag_doc_id in service._rag_instances:
                    try:
                        # ç›´æ¥è°ƒç”¨ finalize_storages() å¹¶ç­‰å¾…å®Œæˆï¼Œç¡®ä¿èµ„æºçœŸæ­£é‡Šæ”¾
                        await service._rag_instances[rag_doc_id].finalize_storages()
                        del service._rag_instances[rag_doc_id]
                        logger.info(f"ğŸ§¹ Cleaned up RAG instance for {rag_doc_id}")
                    except Exception as cleanup_error:
                        logger.warning(f"âš ï¸  Failed to cleanup RAG instance: {cleanup_error}")
                else:
                    logger.info(f"â„¹ï¸  No RAG instance to clean for {rag_doc_id}")
                
                # æ¯ä¸ªæ–‡æ¡£å¤„ç†åæ¸…ç† GPU ç¼“å­˜ï¼Œé˜²æ­¢å†…å­˜æ³„æ¼ç´¯ç§¯
                # è®°å½•æ¸…ç†å‰åçš„ GPU å†…å­˜ï¼ŒéªŒè¯æ˜¯å¦çœŸçš„é‡Šæ”¾äº†
                try:
                    import torch
                    if torch.cuda.is_available():
                        # è®°å½•æ¸…ç†å‰çš„å†…å­˜
                        mem_before = torch.cuda.memory_allocated(0) / 1024**3  # GB
                        reserved_before = torch.cuda.memory_reserved(0) / 1024**3  # GB
                        
                        torch.cuda.empty_cache()
                        
                        # è®°å½•æ¸…ç†åçš„å†…å­˜
                        mem_after = torch.cuda.memory_allocated(0) / 1024**3  # GB
                        reserved_after = torch.cuda.memory_reserved(0) / 1024**3  # GB
                        
                        freed = reserved_before - reserved_after
                        logger.info(
                            f"ğŸ§¹ GPU cache cleared after doc {doc_id}: "
                            f"Allocated {mem_before:.2f}â†’{mem_after:.2f} GB, "
                            f"Reserved {reserved_before:.2f}â†’{reserved_after:.2f} GB "
                            f"(freed {freed:.2f} GB)"
                        )
                except Exception as cache_error:
                    logger.warning(f"âš ï¸  Failed to clear GPU cache: {cache_error}")
                
            except Exception as e:
                import traceback
                logger.error(f"âŒ [{doc_id}] Error: {e}")
                logger.error(f"âŒ [{doc_id}] Traceback:\n{traceback.format_exc()}")
                # å³ä½¿å‡ºé”™ä¹Ÿå°è¯•æ¸…ç†
                if 'rag_doc_id' in locals() and rag_doc_id in service._rag_instances:
                    try:
                        await service._rag_instances[rag_doc_id].finalize_storages()
                        del service._rag_instances[rag_doc_id]
                    except:
                        pass
                continue
    
    logger.info(f"\n{'='*80}")
    logger.info(f"âœ… Answer generation complete!")
    logger.info(f"ğŸ“ Saved to: {output_file}")
    logger.info(f"{'='*80}")


# ==========================================
# Step 2: Evaluate Answers
# ==========================================

async def evaluate_answers(resume: bool = True):
    """
    ä½¿ç”¨ Qwen2.5-32B è¯„ä¼°ç³»ç»Ÿç­”æ¡ˆ
    
    å‰æï¼šQwen2.5-32B æœåŠ¡å·²åœ¨ 8002 ç«¯å£è¿è¡Œ
    
    Args:
        resume: æ˜¯å¦è·³è¿‡å·²è¯„ä¼°çš„ç­”æ¡ˆ
    """
    input_file = OUTPUT_DIR / "system_answers.jsonl"
    output_file = OUTPUT_DIR / "eval_results.jsonl"
    
    if not input_file.exists():
        logger.error(f"âŒ Input file not found: {input_file}")
        logger.info("ğŸ’¡ Please run: python evaluate.py --mode generate first")
        return
    
    # åŠ è½½è¯„ä¼° prompt
    eval_prompt_file = SCRIPT_DIR / "evaluation_prompt.txt"
    with open(eval_prompt_file, 'r', encoding='utf-8') as f:
        eval_prompt = f.read()
    
    # åŠ è½½ç³»ç»Ÿç­”æ¡ˆ
    with open(input_file, 'r', encoding='utf-8') as f:
        answers = [json.loads(line) for line in f]
    
    # åŠ è½½å·²è¯„ä¼°çš„ç­”æ¡ˆï¼ˆç”¨äºæ–­ç‚¹ç»­ä¼ ï¼‰
    evaluated_keys = set()
    if resume and output_file.exists():
        with open(output_file, 'r', encoding='utf-8') as f:
            for line in f:
                data = json.loads(line)
                key = f"{data['doc_id']}_{data['question']}"
                evaluated_keys.add(key)
        logger.info(f"ğŸ“ Resume: found {len(evaluated_keys)} evaluated answers")
    
    # åˆå§‹åŒ– Judge å®¢æˆ·ç«¯
    judge_client = AsyncOpenAI(
        api_key="EMPTY",
        base_url=JUDGE_API_BASE
    )
    
    logger.info(f"\n{'='*80}")
    logger.info(f"âš–ï¸  Evaluating {len(answers)} answers using {JUDGE_MODEL_NAME}")
    logger.info(f"{'='*80}\n")
    
    # è¯„ä¼°æ¯ä¸ªç­”æ¡ˆ
    with open(output_file, 'a', encoding='utf-8') as f_out:
        for i, item in enumerate(answers, 1):
            key = f"{item['doc_id']}_{item['question']}"
            
            # è·³è¿‡å·²è¯„ä¼°
            if resume and key in evaluated_keys:
                logger.info(f"â­ï¸  [{i}/{len(answers)}] Already evaluated, skipping")
                continue
            
            logger.info(f"\n[{i}/{len(answers)}] Doc {item['doc_id']}")
            logger.info(f"  Q: {item['question'][:60]}...")
            logger.info(f"  A: {item['sys_ans'][:60]}...")
            
            # æ„å»ºè¯„ä¼° prompt
            cur_prompt = eval_prompt.replace('{{question}}', item['question']) \
                                     .replace('{{sys_ans}}', item['sys_ans']) \
                                     .replace('{{ref_ans}}', item['ref_ans']) \
                                     .replace('{{ref_text}}', item['evidence'])
            
            try:
                # è°ƒç”¨ Judge æ¨¡å‹
                response = await judge_client.chat.completions.create(
                    model=JUDGE_MODEL_NAME,
                    messages=[
                        {"role": "system", "content": "You are a helpful evaluator."},
                        {"role": "user", "content": cur_prompt}
                    ],
                    temperature=0.0,
                    max_tokens=50
                )
                
                eval_result = response.choices[0].message.content.strip()
                
                # è§£æåˆ†æ•°ï¼ˆåœ¨å‰20ä¸ªå­—ç¬¦ä¸­æŸ¥æ‰¾ 0 æˆ– 1ï¼‰
                score = 1 if '1' in eval_result[:20] else 0
                logger.info(f"  âš–ï¸  Score: {score} | {eval_result[:40]}...")
                
                # ä¿å­˜è¯„ä¼°ç»“æœ
                result = {
                    **item,
                    'eval': eval_result,
                    'score': score
                }
                f_out.write(json.dumps(result, ensure_ascii=False) + '\n')
                f_out.flush()
                
            except Exception as e:
                logger.error(f"  âŒ Error: {e}")
                # ä¿å­˜é”™è¯¯ç»“æœï¼ˆscore=0ï¼‰
                result = {
                    **item,
                    'eval': f"[ERROR: {str(e)}]",
                    'score': 0
                }
                f_out.write(json.dumps(result, ensure_ascii=False) + '\n')
                f_out.flush()
    
    logger.info(f"\n{'='*80}")
    logger.info(f"âœ… Evaluation complete!")
    logger.info(f"ğŸ“ Saved to: {output_file}")
    logger.info(f"{'='*80}")


# ==========================================
# Step 3: Calculate Statistics
# ==========================================

def calculate_statistics():
    """è®¡ç®—è¯„ä¼°ç»Ÿè®¡æ•°æ®"""
    result_file = OUTPUT_DIR / "eval_results.jsonl"
    
    if not result_file.exists():
        logger.error(f"âŒ Result file not found: {result_file}")
        logger.info("ğŸ’¡ Please run: python evaluate.py --mode evaluate first")
        return
    
    # åŠ è½½è¯„ä¼°ç»“æœ
    with open(result_file, 'r', encoding='utf-8') as f:
        results = [json.loads(line) for line in f]
    
    logger.info(f"\n{'='*80}")
    logger.info(f"ğŸ“Š DocBench Evaluation Statistics")
    logger.info(f"{'='*80}\n")
    
    # æ€»ä½“å‡†ç¡®ç‡
    total = len(results)
    correct = sum(1 for r in results if r.get('score', 0) == 1)
    overall_acc = correct / total * 100 if total > 0 else 0
    
    logger.info(f"Overall Accuracy: {overall_acc:.2f}% ({correct}/{total})")
    
    # æŒ‰é—®é¢˜ç±»å‹ç»Ÿè®¡
    type_stats = {}
    for r in results:
        qtype = r['type']
        if qtype not in type_stats:
            type_stats[qtype] = {'correct': 0, 'total': 0}
        type_stats[qtype]['total'] += 1
        if r.get('score', 0) == 1:
            type_stats[qtype]['correct'] += 1
    
    logger.info(f"\nğŸ“‹ Accuracy by Question Type:")
    for qtype in sorted(type_stats.keys()):
        stats = type_stats[qtype]
        acc = stats['correct'] / stats['total'] * 100 if stats['total'] > 0 else 0
        logger.info(f"  {qtype:20s}: {acc:5.2f}% ({stats['correct']:3d}/{stats['total']:3d})")
    
    # æŒ‰é¢†åŸŸç»Ÿè®¡ï¼ˆå‚è€ƒ DocBench å®˜æ–¹åˆ†å¸ƒï¼‰
    domain_ranges = {
        'Academic': range(0, 49),
        'Finance': range(49, 89),
        'Government': range(89, 133),
        'Law': range(133, 179),
        'News': range(179, 229)
    }
    
    domain_stats = {domain: {'correct': 0, 'total': 0} for domain in domain_ranges.keys()}
    for r in results:
        try:
            doc_num = int(r['doc_id'])
            for domain, id_range in domain_ranges.items():
                if doc_num in id_range:
                    domain_stats[domain]['total'] += 1
                    if r.get('score', 0) == 1:
                        domain_stats[domain]['correct'] += 1
                    break
        except:
            continue
    
    logger.info(f"\nğŸŒ Accuracy by Domain:")
    for domain in ['Academic', 'Finance', 'Government', 'Law', 'News']:
        stats = domain_stats[domain]
        if stats['total'] > 0:
            acc = stats['correct'] / stats['total'] * 100
            logger.info(f"  {domain:15s}: {acc:5.2f}% ({stats['correct']:3d}/{stats['total']:3d})")
    
    # ä¿å­˜ç»Ÿè®¡åˆ° JSON
    stats_output = {
        'overall': {'accuracy': overall_acc, 'correct': correct, 'total': total},
        'by_type': {
            qtype: {
                'accuracy': stats['correct'] / stats['total'] * 100 if stats['total'] > 0 else 0,
                'correct': stats['correct'],
                'total': stats['total']
            }
            for qtype, stats in type_stats.items()
        },
        'by_domain': {
            domain: {
                'accuracy': stats['correct'] / stats['total'] * 100 if stats['total'] > 0 else 0,
                'correct': stats['correct'],
                'total': stats['total']
            }
            for domain, stats in domain_stats.items() if stats['total'] > 0
        }
    }
    
    stats_file = OUTPUT_DIR / "statistics.json"
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump(stats_output, f, indent=2, ensure_ascii=False)
    
    logger.info(f"\nğŸ“ Statistics saved to: {stats_file}")
    logger.info(f"{'='*80}\n")


# ==========================================
# Main Entry Point
# ==========================================

async def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description="DocBench Evaluation Script (Manual Server Mode)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # ç”Ÿæˆæ‰€æœ‰æ–‡æ¡£çš„ç­”æ¡ˆ
  python evaluate.py --mode generate
  
  # ç”Ÿæˆéƒ¨åˆ†æ–‡æ¡£çš„ç­”æ¡ˆ
  python evaluate.py --mode generate --start_id 0 --end_id 10
  
  # è¯„ä¼°æ‰€æœ‰ç­”æ¡ˆ
  python evaluate.py --mode evaluate
  
  # æŸ¥çœ‹ç»Ÿè®¡
  python evaluate.py --mode stats
  
  # åå°è¿è¡Œï¼ˆæ¨èï¼‰
  nohup python evaluate.py --mode generate > run.log 2>&1 &
  tail -f run.log
        """
    )
    
    parser.add_argument(
        '--mode',
        type=str,
        required=True,
        choices=['generate', 'evaluate', 'stats'],
        help='è¿è¡Œæ¨¡å¼'
    )
    parser.add_argument(
        '--start_id',
        type=int,
        default=0,
        help='èµ·å§‹æ–‡æ¡£ ID'
    )
    parser.add_argument(
        '--end_id',
        type=int,
        default=229,
        help='ç»“æŸæ–‡æ¡£ IDï¼ˆä¸åŒ…å«ï¼‰'
    )
    parser.add_argument(
        '--no_resume',
        action='store_true',
        help='ä¸æ¢å¤ä¹‹å‰çš„è¿›åº¦ï¼Œä»å¤´å¼€å§‹'
    )
    parser.add_argument(
        '--dump_raw_prompt',
        action='store_true',
        help='ç”Ÿæˆæ¨¡å¼ä¸‹è½ç›˜æ¯é¢˜ raw_promptï¼ˆç”¨äºæ£€æŸ¥ Reference Document Listï¼‰'
    )
    parser.add_argument(
        '--dump_final_messages',
        action='store_true',
        help='ç”Ÿæˆæ¨¡å¼ä¸‹è½ç›˜æœ€ç»ˆå‘ç»™ VLM çš„ messagesï¼ˆç”¨äºæ£€æŸ¥çœŸå®è¾“å…¥ï¼‰'
    )
    
    args = parser.parse_args()
    
    logger.info(f"\n{'='*80}")
    logger.info(f"ğŸ¯ DocBench Evaluation - Manual Server Mode")
    logger.info(f"{'='*80}")
    logger.info(f"Mode: {args.mode}")
    logger.info(f"Range: {args.start_id} to {args.end_id-1}")
    logger.info(f"Resume: {not args.no_resume}")
    logger.info(f"DumpRawPrompt: {args.dump_raw_prompt}")
    logger.info(f"DumpFinalMessages: {args.dump_final_messages}")
    logger.info(f"{'='*80}\n")
    
    # æ‰§è¡Œå¯¹åº”çš„æ­¥éª¤
    if args.mode == 'generate':
        logger.info("âš ï¸  Please ensure InternVL2-26B-AWQ is running on port 8001")
        logger.info(f"   Check: curl http://localhost:8001/v1/models\n")
        await generate_answers(
            start_id=args.start_id,
            end_id=args.end_id,
            resume=not args.no_resume,
            dump_raw_prompt=args.dump_raw_prompt,
            dump_final_messages=args.dump_final_messages,
        )
    
    elif args.mode == 'evaluate':
        logger.info("âš ï¸  Please ensure Qwen2.5-32B is running on port 8002")
        logger.info(f"   Check: curl http://localhost:8002/v1/models\n")
        await evaluate_answers(resume=not args.no_resume)
    
    elif args.mode == 'stats':
        calculate_statistics()
    
    logger.info(f"\nâœ… Done!")


if __name__ == "__main__":
    asyncio.run(main())
